name: Deploy Infrastructure

on:
  #push:
  #  #branches: ["main"]
  #  paths:
  #    - 'etl_pipeline/**'
  #    - 'infra/**'
  #    - 'tests/**'
  #    - '.github/workflows/terraform_deploy.yaml'
  workflow_dispatch:
  #schedule:
  #  - cron: '0 5 * * 1'  # Every Monday at 5:00 AM UTC

jobs:
  test:
    if: false  # MANUAL enabling/disabling of the job
    name: Run Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip install -r etl_pipeline/requirements.txt       
          pip install -r etl_pipeline/requirements-dev.txt

      - name: Run pip-audit
        run: pip-audit

      - name: Run pylint
        run: |
          pylint etl_pipeline --exit-zero
        env:
          PYTHONPATH: .
      
      - name: Run mypy type checks
        run: |
          mypy etl_pipeline
        env:
          PYTHONPATH: .

      - name: Run pytest and generate coverage report
        run: pytest --cov-report=xml
        env:
          PYTHONPATH: .

      - name: Upload coverage to Codacy
        uses: codacy/codacy-coverage-reporter-action@89d6c85cfafaec52c72b6c5e8b2878d33104c699
        with:
          project-token: ${{ secrets.CODACY_PROJECT_TOKEN }}
          coverage-reports: coverage.xml

  deploy:
    if: true  # MANUAL enabling/disabling of the job
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: test  # Waits for test job to pass
    environment: ${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}

    env:
      TF_VAR_project_id: ${{ vars.PROJECT_ID }}
      TF_VAR_sa_email: ${{ vars.TERRAFORM_SERVICE_ACCOUNT_EMAIL }}
      TF_VAR_region: "europe-west9"
      TF_STATE_BUCKET: "${{ vars.PROJECT_ID }}-tf-state"
      TF_STATE_PREFIX: "data-analytics-platform-event-driven"

    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Install zip & Zip code
        run: |
          sudo apt-get update
          sudo apt-get install zip -y
          zip -r ../infra/function-source.zip .
          ls -la ../infra/
        working-directory: etl_pipeline

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@633666f66e0061ca3b725c73b2ec20cd13a8fdd1
        with:
          terraform_version: 1.3.9

      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@6fc4af4b145ae7821d527454aa9bd537d1f2dc5f
        with:
          credentials_json: ${{ secrets.GOOGLE_CREDENTIALS }}

      - name: Cache Terraform plugins
        uses: actions/cache@v3
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-terraform-${{ hashFiles('infra/**/*.tf') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      - name: Terraform Init
        run: |
          terraform init \
            -reconfigure \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="prefix=${TF_STATE_PREFIX}"
        working-directory: infra

      - name: Import Existing Resources
        run: |
          # 1. Import BigQuery dataset surveys if not already in state
          if ! terraform state list | grep -q "google_bigquery_dataset.surveys"; then
            if bq ls --project_id=$TF_VAR_project_id | grep -q "surveys"; then
              echo "Dataset surveys exists but not in state, importing..."
              terraform import google_bigquery_dataset.surveys projects/$TF_VAR_project_id/datasets/surveys
            else
              echo "Dataset surveys does not exist, will be created by Terraform."
            fi
          else
            echo "Dataset surveys is already managed by Terraform, skipping import."
          fi

          # 2. Import BigQuery table developer_salaries if not already in state
          if ! terraform state list | grep -q "google_bigquery_table.developer_salaries"; then
            if bq show --format=prettyjson $TF_VAR_project_id:surveys.developer_salaries > /dev/null 2>&1; then
              echo "BigQuery table developer_salaries exists but not in state, importing..."
              terraform import google_bigquery_table.developer_salaries projects/$TF_VAR_project_id/datasets/surveys/tables/developer_salaries
            else
              echo "BigQuery table developer_salaries does not exist, will be created by Terraform."
            fi
          else
            echo "BigQuery table developer_salaries is already managed by Terraform, skipping import."
          fi

          # 3. Import Service Account if not already in state
          if ! terraform state list | grep -q "google_service_account.function_sa"; then
            if gcloud iam service-accounts describe gcs-to-bq-trigger@$TF_VAR_project_id.iam.gserviceaccount.com --project=$TF_VAR_project_id > /dev/null 2>&1; then
              echo "Service account gcs-to-bq-trigger exists but not in state, importing..."
              terraform import google_service_account.function_sa projects/$TF_VAR_project_id/serviceAccounts/gcs-to-bq-trigger@$TF_VAR_project_id.iam.gserviceaccount.com
            else
              echo "Service account gcs-to-bq-trigger does not exist, will be created by Terraform."
            fi
          else
            echo "Service account function_sa is already managed by Terraform, skipping import."
          fi

          # 4. Import Storage Buckets if not already in state
          if ! terraform state list | grep -q "google_storage_bucket.gcf_source_bucket"; then
            if gsutil ls gs://${TF_VAR_project_id}-gcf-source-bucket > /dev/null 2>&1; then
              echo "Bucket ${TF_VAR_project_id}-gcf-source-bucket exists but not in state, importing..."
              terraform import google_storage_bucket.gcf_source_bucket ${TF_VAR_project_id}-gcf-source-bucket
            else
              echo "Bucket ${TF_VAR_project_id}-gcf-source-bucket does not exist, will be created by Terraform."
            fi
          else
            echo "Bucket gcf_source_bucket is already managed by Terraform, skipping import."
          fi

          if ! terraform state list | grep -q "google_storage_bucket.upload_bucket"; then
            if gsutil ls gs://${TF_VAR_project_id}-upload > /dev/null 2>&1; then
              echo "Bucket ${TF_VAR_project_id}-upload exists but not in state, importing..."
              terraform import google_storage_bucket.upload_bucket ${TF_VAR_project_id}-upload
            else
              echo "Bucket ${TF_VAR_project_id}-upload does not exist, will be created by Terraform."
            fi
          else
            echo "Bucket upload_bucket is already managed by Terraform, skipping import."
          fi

          if ! terraform state list | grep -q "google_storage_bucket.archive_bucket"; then
            if gsutil ls gs://${TF_VAR_project_id}-archive > /dev/null 2>&1; then
              echo "Bucket ${TF_VAR_project_id}-archive exists but not in state, importing..."
              terraform import google_storage_bucket.archive_bucket ${TF_VAR_project_id}-archive
            else
              echo "Bucket ${TF_VAR_project_id}-archive does not exist, will be created by Terraform."
            fi
          else
            echo "Bucket archive_bucket is already managed by Terraform, skipping import."
          fi
        working-directory: infra

      - name: Verify Terraform State
        run: terraform state list
        working-directory: infra

      - name: Terraform Plan
        run: terraform plan -out=tfplan
        working-directory: infra

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
        working-directory: infra

  notify-slack-success:
    needs: deploy
    if: success()
    runs-on: ubuntu-latest

    steps:
      - name: Notify Slack of successful upload
        uses: rtCamp/action-slack-notify@e31e87e03dd19038e411e38ae27cbad084a90661
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_USERNAME: 'GitHub Actions Bot'
          SLACK_COLOR: '#36a64f'  # green
          SLACK_MESSAGE: |
            ✅ *CSV Upload Workflow Succeeded*
            • Repo: `${{ github.repository }}`
            • Branch: `${{ github.ref_name }}`
            • Actor: `${{ github.actor }}`
            • Workflow: `${{ github.workflow }}`
            • Run: <https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}|View logs>

  notify-slack-failure:
    needs: [test, deploy]
    if: failure()
    runs-on: ubuntu-latest

    steps:
      - name: Notify Slack of failed upload
        uses: rtCamp/action-slack-notify@e31e87e03dd19038e411e38ae27cbad084a90661
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_USERNAME: 'GitHub Actions Bot'
          SLACK_COLOR: '#FF0000'  # red
          SLACK_MESSAGE: |
            ❌ *CSV Upload Workflow Failed*
            • Repo: `${{ github.repository }}`
            • Branch: `${{ github.ref_name }}`
            • Actor: `${{ github.actor }}`
            • Workflow: `${{ github.workflow }}`
            • Run: <https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}|View logs>
